
# Unmasking the Truth: AI-Powered Deepfake Image Detection for Public Safety

This project aims to combat the rising threat of deepfakes by leveraging state-of-the-art Vision Transformer (ViT) and CLIP models to detect fake, AI-generated, or manipulated images. With applications in media, law enforcement, branding, and public safety, our solution provides a web-based tool for deepfake image analysis.

We curated a diverse dataset covering multiple deepfake manipulation typesâ€”face swaps, attribute editing, and face synthesisâ€”and evaluated seven pretrained models from Hugging Face. The system generates detailed predictions including class-wise probabilities and confidence scores, and identifies the best-performing model for each manipulation type.

Our user-friendly Streamlit application allows batch uploads, model comparisons, and result summaries to help users verify visual content quickly and accurately.

This project demonstrates a scalable approach to deepfake detection, offering flexibility across different use cases and paving the way for future extensions into video and audio deepfake detection.


## Features

- Upload and analyze multiple images at once
- Choose from 7 pretrained models (ViT, CLIP, etc.)
- Compare predictions across all models
- Detect real, fake, or AI-generated images
- View detailed confidence scores and per-class probabilities
- Light/Dark theme toggle for UI comfort
- Model-wise performance summary based on manipulation type
- User-friendly Streamlit web interface â€” no coding required

## Installation 
Follow these steps to set up and run the project locally:

#### 1.Clone the repository
git clone https://github.com/VRUSH1909/DeepFake_Image_Detection.git
cd DeepFake_Image_Detection

#### 2.Install dependencies
pip install -r requirements.txt

#### 3.Run the app
streamlit run app.py



## Datasets

To build a comprehensive deepfake detection system, we curated a diverse dataset from various trusted open-source platforms. Our goal was to cover multiple types of image manipulation techniques for broader evaluation.

- We collected real and fake images from **Kaggle**, **Hugging Face Datasets**, and **public deepfake repositories**.
- The fake images included:
  - **Face Swapping** (one personâ€™s face replaced with another)
  - **Attribute Editing** (changes in facial features like age, gender, expressions)
  - **Face Synthesis** (AI-generated faces using GANs and other models)
  - **AI-generated animals, scenery, and art**
- Real images were sourced to ensure model comparison could distinguish between genuine and synthetic content.

The dataset was manually inspected and organized into labeled categories to help evaluate how well different models perform on various manipulation types. This structured approach allowed for targeted testing and better insights into model strengths and weaknesses.
## Models Used â€” A Table Listing the Models and Their Tasks

The project uses a variety of pretrained transformer-based models from Hugging Face for deepfake image classification. Each model is specialized to detect different types of manipulations:

| Model Name                                    | Model Type         | Prediction Classes            | Task / Focus Area                         |
|----------------------------------------------|--------------------|-------------------------------|-------------------------------------------|
| `ashish-001/deepfake-detection-using-ViT`    | ViT (Vision Transformer) | Real / Fake               | Best for edited faces (attribute changes) |
| `prithivMLmods/Deep-Fake-Detector-v2-Model`  | ViT                | Real / Fake                   | Performs well on face swap detection      |
| `dima806/ai_vs_real_image_detection`         | ViT                | Real / Fake                   | Identifies AI-generated vs real images    |
| `dima806/deepfake_vs_real_image_detection`   | ViT                | Real / Fake                   | Focused on detecting deepfakes            |
| `openai/clip-vit-base-patch32`               | CLIP (ViT + Text)  | Real / Fake (via prompt)      | Uses text-image comparison                |
| `prithivMLmods/AI-vs-Deepfake-vs-Real`       | ViT                | Real / Fake / AI-generated    | Handles multi-class classification        |
| `prithivMLmods/Deep-Fake-Detector-Model`     | ViT                | Real / Fake                   | General-purpose deepfake detection        |

Each model is integrated dynamically into the Streamlit application and tested on different image categories for comparison.

---

## Results and Evaluation â€” Model Performance Across Categories

The following table summarizes the accuracy (confidence scores) of each model across different deepfake categories such as face swapping, attribute editing, AI art, animals, and scenery. `R` = Real, `F` = Fake, `D` = Deepfake, `A` = AI-generated.

| Categories                    | Deep-Fake-Detector-Model | ashish-001 | AI-vs-Deepfake-vs-Real | CLIP | ai-vs-real-image-detection | deepfake-vs-real-image-detection | Deep-Fake-Detector-v2 |
|------------------------------|--------------------------|------------|-------------------------|------|-----------------------------|----------------------------------|------------------------|
| Face Swapping (Fake)         | R â€“ 71.97%, F â€“ 28.03%   | R â€“ 94.26%, F â€“ 5.74%   | R â€“ 58.65%, D â€“ 26.18%, A â€“ 58.65% | R â€“ 70.49%, F â€“ 29.51% | R â€“ 25.03%, F â€“ 74.97%     | R â€“ 94.73%, F â€“ 5.27%           | R â€“ 24.43%, D â€“ 75.57%       |
| Attribute Editing (Real)     | R â€“ 71.03%, F â€“ 28.97%   | R â€“ 80.61%, F â€“ 20.39%  | R â€“ 42.72%, D â€“ 36.63%, A â€“ 20.66% | R â€“ 77.07%, F â€“ 22.93% | R â€“ 30.94%, F â€“ 69.06%     | R â€“ 72.34%, F â€“ 27.66%          | R â€“ 40.67%, F â€“ 59.33%       |
| Attribute Editing (Fake)     | R â€“ 79.56%, F â€“ 20.44%   | R â€“ 39.88%, F â€“ 75.12%  | R â€“ 32.60%, D â€“ 47.05%, A â€“ 20.35% | R â€“ 65.17%, F â€“ 34.83% | R â€“ 20.29%, F â€“ 79.71%     | R â€“ 39.09%, F â€“ 60.91%          | R â€“ 45.48%, F â€“ 54.52%       |
| Face Synthesis (Fake)        | R â€“ 77.26%, F â€“ 22.74%   | R â€“ 17.01%, F â€“ 82.99%  | R â€“ 43.17%, D â€“ 38.39%, A â€“ 18.45% | R â€“ 63.79%, F â€“ 36.21% | R â€“ 33.85%, F â€“ 66.15%     | R â€“ 43.13%, F â€“ 56.87%          | R â€“ 41.62%, F â€“ 58.38%       |
| Animals (Real)               | R â€“ 56.59%, F â€“ 43.41%   | R â€“ 68.10%, F â€“ 31.90%  | R â€“ 31.54%, D â€“ 34.79%, A â€“ 33.67% | R â€“ 48.23%, F â€“ 51.77% | R â€“ 29.59%, F â€“ 70.41%     | R â€“ 69.13%, F â€“ 30.87%          | R â€“ 42.76%, F â€“ 57.24%       |
| Animals (Fake)               | R â€“ 53.28%, F â€“ 46.72%   | R â€“ 62.78%, F â€“ 37.52%  | R â€“ 24.20%, D â€“ 32.09%, A â€“ 43.71% | R â€“ 41.95%, F â€“ 58.05% | R â€“ 25.41%, F â€“ 74.59%     | R â€“ 79.77%, F â€“ 20.23%          | R â€“ 34.15%, F â€“ 65.85%       |
| Scenery (Real)               | R â€“ 50.90%, F â€“ 49.10%   | R â€“ 40.46%, F â€“ 59.54%  | R â€“ 25.75%, D â€“ 32.39%, A â€“ 41.86% | R â€“ 27.60%, F â€“ 72.40% | R â€“ 8.16%, F â€“ 91.84%      | R â€“ 66.98%, F â€“ 33.02%          | R â€“ 40.86%, F â€“ 59.14%       |
| Scenery (Fake)               | R â€“ 49.30%, F â€“ 50.70%   | R â€“ 59.70%, F â€“ 40.30%  | R â€“ 30.18%, D â€“ 30.89%, A â€“ 38.94% | R â€“ 58.26%, F â€“ 41.74% | R â€“ 14.35%, F â€“ 85.65%     | R â€“ 81.10%, F â€“ 18.90%          | R â€“ 38.21%, F â€“ 61.79%       |
| Art (Human)                  | R â€“ 42.68%, F â€“ 57.32%   | R â€“ 44.03%, F â€“ 55.97%  | R â€“ 27.57%, D â€“ 33.40%, A â€“ 39.03% | R â€“ 3.69%, F â€“ 96.31%  | R â€“ 17.95%, F â€“ 82.05%     | R â€“ 82.54%, F â€“ 17.46%          | R â€“ 36.96%, F â€“ 63.04%       |
| Art (AI-generated)           | R â€“ 46.58%, F â€“ 53.42%   | R â€“ 60.40%, F â€“ 39.60%  | R â€“ 23.24%, D â€“ 32.52%, A â€“ 44.24% | R â€“ 22.48%, F â€“ 77.52% | R â€“ 9.27%, F â€“ 90.73%      | R â€“ 86.67%, F â€“ 13.33%          | R â€“ 34.07%, F â€“ 65.93%       |

These results helped us understand that **no single model is perfect across all categories**. Instead, we recommend using **different models depending on the image type**, or implementing **model ensembling** in future development.

## Usage â€” How to Run the App and Use It

## ğŸš€ Usage

Once the app is running, you can interact with the Deepfake Detection system through a clean and intuitive web interface. Here's how to use it:

1. **Upload Images**  
   - Click the upload area to select one or multiple images (`.jpg`, `.jpeg`, or `.png` format).  
   - You can upload real, fake, or AI-generated images for analysis.

2. **Select a Model**  
   - Choose one specific pretrained model from the dropdown for individual predictions,  
   **or**  
   - Enable the **"Predict using all models"** option to compare results across all supported models.

3. **Get Predictions**  
   - Instantly view predictions for each image, including:
     - **Predicted label** (Real / Fake / AI-generated)
     - **Confidence score**
     - **Per-class probability breakdown**

4. **Review Summary**  
   - After batch analysis, the app provides:
     - **Majority prediction across all images**
     - **Average confidence score**
     - **Model-wise class probability summary**

This tool is designed to be used by journalists, forensic analysts, researchers, and the general publicâ€”no coding knowledge is needed.


## Future Workâ€” Planned Improvements

While the current application effectively detects deepfakes in images using multiple AI models, there are several directions for expanding and enhancing the system:

### ğŸ”¹ 1. Video Deepfake Detection
We plan to extend the solution to detect deepfakes in videos by analyzing individual frames. Since videos are just sequences of images, applying the same detection logic frame-by-frame will allow us to flag manipulated video content. Additional techniques like temporal consistency checks can further improve accuracy.

### ğŸ”¹ 2. Audio Deepfake Analysis
Another major goal is to integrate audio deepfake detection. This would involve identifying synthetically generated voices, altered speech, or impersonation using waveform and spectrogram analysis, possibly leveraging pretrained models such as Wav2Vec or Whisper.

### ğŸ”¹ 3. Report Generation
We aim to add the ability to generate automated, downloadable reports (in PDF format) summarizing:
- Prediction results for uploaded images/videos
- Confidence scores
- Class-wise probability distributions
- Model-wise comparison
This feature would be useful for law enforcement, media teams, or researchers who require audit-ready documentation.

### ğŸ”¹ 4. Cloud Deployment
Plans are in place to deploy the application on platforms like **Streamlit Cloud**, **Hugging Face Spaces**, or even integrate it into a **browser extension** for real-time content validation.

### ğŸ”¹ 5. Model Ensembling
To improve accuracy and reliability, we also intend to implement **model ensembling**â€”where multiple models contribute to a single prediction, using weighted averaging or voting mechanisms based on the image category.

These future improvements will make the system more comprehensive, scalable, and applicable to real-world scenarios involving misinformation, media verification, and public safety.

## License and Credits


### ğŸ” License
This project is intended for educational and research purposes only.  
It uses publicly available pretrained models and open-source datasets for experimentation and evaluation.  
Please ensure compliance with individual model licenses if deploying commercially.

---

### ğŸ‘©â€ğŸ’» Contributors
- **Vrushali Dhage** â€“ Application development,performance analysis, evaluation , documentation.
- **Samiksha Jagtap** â€“ Model research, dataset preparation, evaluation.

---

### ğŸ§‘â€ğŸ« Project Mentors / Guides
- **Kashish Arora** â€“ Technical mentorship, model validation support
- **Rohita Munde** â€“ Project guidance, architecture suggestions


---

We thank all open-source contributors, Hugging Face model creators, and the research community whose tools made this project possible.
